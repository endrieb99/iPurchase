# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lLwbyVDFnijG7eLTllWnjdnBKS3tZFLX

<a href="https://colab.research.google.com/github/endrieb99/ML_SIDE/blob/main/sentiment_analysis.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

# Import the necessary libraries
import re
import tweepy
from tweepy import OAuthHandler
from better_profanity import profanity
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
import pandas as pd
import json
from collections import Counter

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

nltk.download('stopwords')
nltk.download('punkt')

# Initialize Twitter api
def init_twitter_api():
    # API Key
    consumer_key = 'v87i14mp7M9JdCSkd5TVrS8Bo'
    consumer_secret = 'X8dQjpZ5I0x0xGjChMFdIbJ1cgzGe01NfQNxAwZAeqydsT4qoG'
    access_token = '1404028707346563073-3jTvQOpROLk5vd5SMuavvam4ibqzz9'
    access_token_secret = 'w6dV8bKp08S7vUydjDjzHx3zslMPnpvFi7LlkEVw4MLmJ'

    # Access Twitter Data
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)

    api = tweepy.API(auth)
    return api


# Fetch tweets with query from user
def get_tweets_from_api(api_query_count, query):
    # Filter the query to remove retweets
    filtered = query + "-filter:retweets"

    # Generate the latest tweets on the given query
    api = init_twitter_api()
    tweets = tweepy.Cursor(
        api.search_tweets,
        q=filtered,
        lang="en").items(api_query_count)

    tweet_list = []
    for tweet in tweets:
        tweet_list.append([tweet.id, tweet.source_url, tweet.text, tweet.created_at,
                           tweet.retweet_count, tweet.favorite_count, tweet.user.location])

    return tweet_list


def list_to_data_frame(tweet_list):
    # Convert the list into a dataframe
    df = pd.DataFrame(data=tweet_list, columns=['ID', 'Link', 'Tweets',
                                                'Date', 'Retweets',
                                                'Favorites', 'Location'])
    return df


# Create a function to clean the tweets
# Data Preprocessing
def clean_tweets(txt):
    # changing tweet text to lowercase
    txt = str(txt).lower()
    # remove profanity
    txt = profanity.censor(txt)
    txt = re.sub("@[A-Za-z0-9_]+", "", txt)
    txt = re.sub("#[A-Za-z0-9_]+", "", txt)
    txt = re.sub(r'http\S+', '', txt)
    txt = re.sub('[()!?]', ' ', txt)
    txt = re.sub('\[.*?\]', ' ', txt)
    txt = re.sub("[^a-z0-9]", " ", txt)
    txt = txt.split()
    txt = " ".join(word for word in txt)

    return txt


# Handle stopwords
def remove_stop_word(txt):
    stop_words = stopwords.words('english')
    stopwords_dict = Counter(stop_words)
    text = " ".join([word for word in txt.split() if word not in stopwords_dict])
    return text


def add_tweet_polarity_score_compound(tweet):
    analyser = SentimentIntensityAnalyzer()

    score = analyser.polarity_scores(tweet)
    compound = score['compound']
    negative = score['neg']
    neutral = score['neu']
    positive = score['pos']

    return compound


def polarity_to_score(compound):
    score = (compound + 1) * 5
    return score


# Defining a function to assign sentiments (positive, negative or neutral)
def classify_tweets_polarity(compound):
    # The Compound score is a metric that has been scaled between -1 as the most extreme negative
    # score and +1 as the most extreme positive score positive sentiment = compound score >= 0.05
    # neutral sentiment = (compound score > -0.05) and (compound score < 0.05) negative sentiment
    # = compound score <= -0.05

    if compound <= -0.05:
        return "Negative"
    elif -0.05 > compound < 0.05:
        return "Neutral"
    else:
        return "Positive"


def extract_tweet_sentiments(data_frame, column):
    # Extracting tweets
    tweets = data_frame[data_frame.Polarity == column]
    tweets = tweets.sort_values(["Polarity"], ascending=True)
    return tweets


def get_compound_average(data_frame):
    compound_average = sum(data_frame['Compound']) / len(data_frame['Compound'])
    formatted_compound_average = '{0:.2f}'.format(compound_average)
    return formatted_compound_average


def get_ip_score(data_frame):
    score = sum(data_frame['Score']) / len(data_frame['Score'])
    formatted_score = '{0:.2f}'.format(score)
    return formatted_score


def get_purchase_advice(compound):
    compound_to_float = float(compound)
    if compound_to_float >= 0.5:
        return "Based on comments and sentiments analysed from Social Media, it appears that the " \
               "overall opinion of this product is highly positive. \n\nThis means that most of " \
               "reviews express a positive emotion towards the product. This suggest that the " \
               "product is likely to have a good overall quality and satisfy ones need. \n\nYou " \
               "should consider purchasing it. "
    elif -0.5 < compound_to_float < 0.5:
        return "Based on comments and sentiments analysed from Social Media, it appears that the " \
               "overall opinion of this product is neutral. \n\nThis means that the amount of " \
               "positive and negative comments about the product are balanced. You should " \
               "carefully consider the product before making a purchase. \n\nLook for additional " \
               "information about the product, read reviews from different sources, compare it " \
               "with other similar products to weigh the pros and cons. "
    else:
        return "Based on comments and sentiments analysed from Social Media, it appears that the " \
               "overall opinion of this product is highly negative. \n\nThis means that most of " \
               "the comments and reviews express a negative emotion towards the product. This " \
               "suggest that the product may have some issues based on your keyphrase or it may " \
               "not satisfy ones overall needs. \n\nYou should avoid purchasing it, or do a more " \
               "thorough research before making a decision. "


def get_frequent_words_list(column):
    # Convert the column to a list
    text_list = column.tolist()

    # Join all the words in the list into a single string
    text_string = " ".join(text_list)

    # Tokenize the string into a list of words
    text_list = word_tokenize(text_string)

    # Use the Counter class to get the frequency of each word
    word_count = Counter(text_list)

    # Get the most frequent words
    most_common_words = word_count.most_common(25)

    # Get the most frequent words list without frequency
    most_common_words_list = [word for word, frequency in most_common_words]

    return most_common_words_list


def get_frequent_words(positive_words, negative_words, neutral_words):
    return {'positive': positive_words, 'negative': negative_words, 'neutral': neutral_words}


def get_tweets(positive_tweets, negative_tweets, neutral_tweets):
    return {'positive': positive_tweets[0:20].to_dict('records'),
            'negative': negative_tweets[0:20].to_dict('records'),
            'neutral': neutral_tweets[0:20].to_dict('records')}


def generate_report_json(api_query_count, ip_score, purchase_advice, frequent_words, tweets):
    return {'Report': {'APIQueryCount': api_query_count, 'IPScore': ip_score,
                       'PurchaseAdvice': purchase_advice, 'FrequentWords': frequent_words,
                       'Tweets': tweets}}


def convert_dictionary_to_json(report):
    json_object = json.dumps(report, indent=4, sort_keys=True, default=str)
    return json_object


def get_sentiment_analysis_report(api_query_count, query):
    # Step 1 --- Fetch Tweet List
    # Define how many tweets to get from the Twitter API
    tweet_list = get_tweets_from_api(api_query_count, query)
    tweet_list_count = len(tweet_list)

    if not tweet_list_count:
        final_report = convert_dictionary_to_json({'Report': None})
        return final_report

    # Step 2 --- Convert list to Data Frame
    data_frame = list_to_data_frame(tweet_list)

    # Step 3 --- Clean tweets
    data_frame['Tweets'] = data_frame['Tweets'].apply(lambda tw: clean_tweets(tw))

    # Step 4 --- Remove Stop Words
    data_frame['Tweets'] = data_frame['Tweets'].apply(lambda tw: remove_stop_word(tw))

    # Step 5 --- Determine polarity scores
    data_frame['Compound'] = data_frame['Tweets'].apply(add_tweet_polarity_score_compound)

    # Step 6 --- Convert polarity to Score
    data_frame['Score'] = data_frame['Compound'].apply(polarity_to_score)

    # Step 7 --- Classify tweets: assign sentiments
    data_frame['Polarity'] = data_frame['Compound'].apply(classify_tweets_polarity)
    data_frame.duplicated().sum()
    data_frame.isnull().sum().sum()
    data_frame['Polarity'].value_counts()

    # Step 8 --- Extract sentiments from tweets
    positive_tweets = extract_tweet_sentiments(data_frame, "Positive")
    negative_tweets = extract_tweet_sentiments(data_frame, "Negative")
    neutral_tweets = extract_tweet_sentiments(data_frame, "Neutral")

    # Step 9 --- Get frequent words from word_cloud
    positive_words = get_frequent_words_list(positive_tweets['Tweets'])
    negative_words = get_frequent_words_list(negative_tweets['Tweets'])
    neutral_words = get_frequent_words_list(neutral_tweets['Tweets'])

    # Step 11 --- Generate report dictionary
    ip_score = get_ip_score(data_frame)
    compound_average = get_compound_average(data_frame)
    purchase_advice = get_purchase_advice(compound_average)
    frequent_words = get_frequent_words(positive_words, negative_words, neutral_words)
    tweets = get_tweets(positive_tweets, negative_tweets, neutral_tweets)

    report = generate_report_json(api_query_count, ip_score, purchase_advice, frequent_words,
                                  tweets)

    # Step 12 --- Convert dictionary to json
    final_report = convert_dictionary_to_json(report)

    return final_report
